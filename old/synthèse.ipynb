{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisation des résultats\n",
    "# Load the test dataset\n",
    "test_dataset = MyDataset('test.hdf5')\n",
    "print(batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Chemin vers les modèles sauvergardés\n",
    "chemin_vers_sauvegarde_model0 = 'model0.pth'\n",
    "chemin_vers_sauvegarde_model1 = 'model1.pth'\n",
    "chemin_vers_sauvegarde_model2 = 'model2.pth'\n",
    "chemin_vers_sauvegarde_model3 = 'model3.pth'\n",
    "chemin_vers_sauvegarde_model4 = 'model4.pth'\n",
    "chemin_vers_sauvegarde_modelLSTM = 'modelCNNLSTM.pth'\n",
    "chemin_vers_sauvegarde_model3_augmented = 'model3_augmented.pth'\n",
    "\n",
    "# Load the trained models\n",
    "model0 = torch.load(chemin_vers_sauvegarde_model0, map_location=device, weights_only=False)\n",
    "model1 = torch.load(chemin_vers_sauvegarde_model1, map_location=device, weights_only=False)\n",
    "model2 = torch.load(chemin_vers_sauvegarde_model2, map_location=device, weights_only=False)\n",
    "model3 = torch.load(chemin_vers_sauvegarde_model3, map_location=device, weights_only=False)\n",
    "model4 = torch.load(chemin_vers_sauvegarde_model4, map_location=device, weights_only=False)\n",
    "model_lstm = torch.load(chemin_vers_sauvegarde_modelLSTM, map_location=device, weights_only=False)\n",
    "model3_augmented = torch.load(chemin_vers_sauvegarde_model3_augmented, map_location=device, weights_only=False)\n",
    "\n",
    "# Set the models to evaluation mode\n",
    "model0.eval()\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "model_lstm.eval()\n",
    "model3_augmented.eval()\n",
    "\n",
    "# Move models to the appropriate device\n",
    "model0.to(device)\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "model4.to(device)\n",
    "model_lstm.to(device)\n",
    "model3_augmented.to(device)\n",
    "\n",
    "# Initialize accuracy metric\n",
    "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "\n",
    "# Initialize lists to store true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels_model0 = []\n",
    "predicted_labels_model1 = []\n",
    "predicted_labels_model2 = []\n",
    "predicted_labels_model3 = []\n",
    "predicted_labels_model4 = []\n",
    "predicted_labels_modelLSTM = []\n",
    "predicted_labels_model3_augmented = []\n",
    "\n",
    "# Evaluate models\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        # Ensure the input tensor has the correct shape\n",
    "        if batch_x.shape[1] == 2048:\n",
    "            batch_x = batch_x.view(batch_x.size(0), 2, -1)\n",
    "        \n",
    "        # Collect true labels\n",
    "        true_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for model, predicted_labels in zip(\n",
    "            [model0, model1, model2, model3, model4, model_lstm, model3_augmented],\n",
    "            [predicted_labels_model0, predicted_labels_model1, predicted_labels_model2, predicted_labels_model3, predicted_labels_model4, predicted_labels_modelLSTM, predicted_labels_model3_augmented]\n",
    "        ):\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays for easier manipulation and faster computing\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels_model0 = np.array(predicted_labels_model0)\n",
    "predicted_labels_model1 = np.array(predicted_labels_model1)\n",
    "predicted_labels_model2 = np.array(predicted_labels_model2)\n",
    "predicted_labels_model3 = np.array(predicted_labels_model3)\n",
    "predicted_labels_model4 = np.array(predicted_labels_model4)\n",
    "predicted_labels_modelLSTM = np.array(predicted_labels_modelLSTM)\n",
    "predicted_labels_model3_augmented = np.array(predicted_labels_model3_augmented)\n",
    "\n",
    "# Calculate accuracy for each model\n",
    "print(predicted_labels_model0)\n",
    "accuracy_model0 = accuracy_metric(torch.tensor(predicted_labels_model0, dtype=torch.float32), torch.tensor(true_labels)).item()\n",
    "accuracy_model1 = accuracy_metric(torch.tensor(predicted_labels_model1), torch.tensor(true_labels)).item()\n",
    "accuracy_model2 = accuracy_metric(torch.tensor(predicted_labels_model2), torch.tensor(true_labels)).item()\n",
    "accuracy_model3 = accuracy_metric(torch.tensor(predicted_labels_model3), torch.tensor(true_labels)).item()\n",
    "accuracy_model4 = accuracy_metric(torch.tensor(predicted_labels_model4), torch.tensor(true_labels)).item()\n",
    "accuracy_modelLSTM = accuracy_metric(torch.tensor(predicted_labels_modelLSTM), torch.tensor(true_labels)).item()\n",
    "accuracy_model3_augmented = accuracy_metric(torch.tensor(predicted_labels_model3_augmented), torch.tensor(true_labels)).item()\n",
    "\n",
    "# Print accuracies\n",
    "print(f\"Model0 Accuracy: {accuracy_model0:.4f}\")\n",
    "print(f\"Model1 Accuracy: {accuracy_model1:.4f}\")\n",
    "print(f\"Model2 Accuracy: {accuracy_model2:.4f}\")\n",
    "print(f\"Model3 Accuracy: {accuracy_model3:.4f}\")\n",
    "print(f\"Model4 Accuracy: {accuracy_model4:.4f}\")\n",
    "print(f\"ModelLSTM Accuracy: {accuracy_modelLSTM:.4f}\")\n",
    "print(f\"Model3 Augmented Accuracy: {accuracy_model3_augmented:.4f}\")\n",
    "\n",
    "# Calculate confusion matrices for each model\n",
    "#confusion_matrix_model0 = confusion_matrix(true_labels, predicted_labels_model0)\n",
    "confusion_matrix_model1 = confusion_matrix(true_labels, predicted_labels_model1)\n",
    "confusion_matrix_model2 = confusion_matrix(true_labels, predicted_labels_model2)\n",
    "confusion_matrix_model3 = confusion_matrix(true_labels, predicted_labels_model3)\n",
    "confusion_matrix_model4 = confusion_matrix(true_labels, predicted_labels_model4)\n",
    "confusion_matrix_modelLSTM = confusion_matrix(true_labels, predicted_labels_modelLSTM)\n",
    "confusion_matrix_model3_augmented = confusion_matrix(true_labels, predicted_labels_model3_augmented)\n",
    "\n",
    "# Plot confusion matrices\n",
    "for model_name, cm in zip(\n",
    "    [\"Model0\", \"Model1\", \"Model2\", \"Model3\", \"Model4\", \"ModelLSTM\", \"Model3 Augmented\"],\n",
    "    #[confusion_matrix_model0, confusion_matrix_model1, confusion_matrix_model2, confusion_matrix_model3, confusion_matrix_model4, confusion_matrix_modelLSTM, confusion_matrix_model3_augmented]\n",
    "    [confusion_matrix_model1, confusion_matrix_model2, confusion_matrix_model3, confusion_matrix_model4, confusion_matrix_modelLSTM, confusion_matrix_model3_augmented]\n",
    "):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy vs SNR\n",
    "accuracies = {\n",
    "    \"model1\": [],\n",
    "    \"model2\": [],\n",
    "    \"model3\": [],\n",
    "    \"model4\": [],\n",
    "    \"modelLSTM\": [],\n",
    "    \"model3_augmented\": []\n",
    "}\n",
    "snrs = distinct_snr\n",
    "\n",
    "for snr in snrs:\n",
    "    true_labels_snr = []\n",
    "    predicted_labels_snr = {model: [] for model in accuracies.keys()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Ensure the input tensor has the correct shape\n",
    "            if batch_x.shape[1] == 2048:\n",
    "                batch_x = batch_x.view(batch_x.size(0), 2, -1)\n",
    "            \n",
    "            # Collect true labels\n",
    "            true_labels_snr.extend(batch_y.cpu().numpy())\n",
    "            \n",
    "            # Evaluate each model\n",
    "            for model_name, model in zip(accuracies.keys(), \n",
    "                                         [model1, model2, model3, model4, model_lstm, model3_augmented]):\n",
    "                outputs = model(batch_x)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                predicted_labels_snr[model_name].extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculate accuracy for each model at the current SNR\n",
    "        for model_name in accuracies.keys():\n",
    "            print(model_name)\n",
    "            acc = accuracy_metric(torch.tensor(predicted_labels_snr[model_name], dtype=torch.float32), \n",
    "                                  torch.tensor(true_labels_snr))\n",
    "            accuracies[model_name].append(acc.item())\n",
    "\n",
    "# Plot accuracy vs SNR\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_name, model_accuracies in accuracies.items():\n",
    "    plt.plot(snrs, model_accuracies, label=model_name)\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs SNR for Different Models\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "complexities = {\n",
    "    \"Model0\": count_parameters(model0),\n",
    "    \"Model1\": count_parameters(model1),\n",
    "    \"Model2\": count_parameters(model2),\n",
    "    \"Model3\": count_parameters(model3),\n",
    "    \"Model4\": count_parameters(model4),\n",
    "    \"ModelLSTM\": count_parameters(model_lstm),\n",
    "    \"Model3 Augmented\": count_parameters(model3_augmented)\n",
    "}\n",
    "\n",
    "print(\"Model Complexities:\")\n",
    "for model_name, num_params in complexities.items():\n",
    "    print(f\"{model_name}: {num_params} parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
