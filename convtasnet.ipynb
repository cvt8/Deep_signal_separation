{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.axes._axes import Axes\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import stft,istft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=6><b>Meta paramètres</b></font>\n",
    "\n",
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute puis les sauvegarder sur le disque. Toutefois, <font color=\"red\"><b>ceci triple l'espace occupé sur disque</b></font>: passant de 6.3 Go à 17.2 Go. Veuillez donc préciser le paramètre `SAVE_SPECTROGRAMS` selon si vous pouvez utiliser cet espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = Path(\"source_separation\")\n",
    "SAVE_SPECTROGRAMS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des signaux, Visualisation et Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des signaux et spectrogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = DIRECTORY / \"train\"\n",
    "train_small_folder = DIRECTORY / \"train_small\"\n",
    "test_folder = DIRECTORY / \"test\"\n",
    "get_path = lambda folder,i : folder / (\"000\"+str(i))[-4:]\n",
    "\n",
    "datasets_sizes = {\n",
    "    train_small_folder : 50,\n",
    "    train_folder : 5000,\n",
    "    test_folder : 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les signaux ont la même fréquence d'échantillonage, même longueur; et donc les mêmes f et t échantillonés pour le Spectrogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_same(folder=train_small_folder):\n",
    "    f_ref = None\n",
    "    t_ref = None\n",
    "    for i in range(datasets_sizes[train_small_folder]):\n",
    "        fe,signal = wavfile.read(get_path(train_small_folder,i) / \"voice.wav\")\n",
    "        len_signal = len(signal)\n",
    "        f_spec,t_spec,spec = stft(\n",
    "            signal,fs=fe,\n",
    "            nperseg=400,nfft=512,noverlap=100)\n",
    "        if f_ref is None: f_ref = f_spec ; t_ref = t_spec\n",
    "        assert np.allclose(f_spec,f_ref)\n",
    "        assert np.allclose(t_spec,t_ref)\n",
    "    return fe,f_ref,t_ref\n",
    "\n",
    "fe,f_ref,t_ref = check_all_same()\n",
    "get_spectrogram = lambda signal : stft(signal,fs=fe,nperseg=400,nfft=512,noverlap=100)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal_folder(folder: Path,\n",
    "        load_signals=True,\n",
    "        load_spectrograms=True) -> dict[str,dict]:\n",
    "    \"\"\"\n",
    "    Return a dictionary with 3 sub dicts: \"voice\", \"noise\" and \"mix\"; and an \"SNR\" key.\n",
    "    Each sub dict has 3 keys: \"filename\", \"signal\" and \"spectrogram\" \n",
    "    (except if load_signals or load_spectrograms are set to False)\n",
    "    \"\"\"\n",
    "    keys = [\"voice\",\"noise\",\"mix\"]\n",
    "    res = dict((k,dict()) for k in keys)\n",
    "    for f in folder.iterdir():\n",
    "        assert f.is_file()\n",
    "        if \"voice\" in f.name: key = \"voice\"\n",
    "        elif \"noise\" in f.name: key = \"noise\"\n",
    "        else: \n",
    "            key = \"mix\"\n",
    "            if f.suffix == \".wav\":\n",
    "                res[\"SNR\"] = f.name.removesuffix(\".wav\").split(\"_\")[-1]\n",
    "        if f.suffix == \".wav\" and load_signals:\n",
    "            fe,signal = wavfile.read(f)\n",
    "            res[key][\"filename\"] = f.name\n",
    "            res[key][\"signal\"] = signal\n",
    "        elif f.suffix == \".pt\" and SAVE_SPECTROGRAMS and load_spectrograms: \n",
    "            # when SAVE_SPECTROGRAMS is False, we shouldn't be able to load them \n",
    "            # to save time, otherwise it's cheating.\n",
    "            res[key][\"spectrogram\"] = torch.load(f)\n",
    "    # Create missing spectrograms\n",
    "    if load_spectrograms:\n",
    "        for key in keys:\n",
    "            if \"spectrogram\" not in res[key]:\n",
    "                assert load_signals\n",
    "                spec = get_spectrogram(res[key][\"signal\"])\n",
    "                res[key][\"spectrogram\"] = spec\n",
    "                if SAVE_SPECTROGRAMS:\n",
    "                    torch.save(torch.tensor(spec),folder / f\"{key}_spectrogram.pt\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_all_spectrograms():\n",
    "    for folder in [train_folder,train_small_folder,test_folder]:\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            folder_i: Path = get_path(folder,i)\n",
    "            for f in folder_i.iterdir():\n",
    "                if \"spectrogram\" in f.name:\n",
    "                    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute, en sauvegardant tous les spectrogrammes sur disque. Attention, <font color=\"red\"><b>ceci triple l'espace occupé sur disque</b></font>: passant de 6.3 Go à 17.2 Go.\n",
    "Pour les retirer utiliser: `remove_all_spectrograms()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrograms(folder: Path):\n",
    "    if SAVE_SPECTROGRAMS and not (folder/\"0000\"/\"voice_spectrogram.pt\").exists():\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            load_signal_folder(get_path(folder,i))\n",
    "\n",
    "create_spectrograms(train_small_folder)\n",
    "create_spectrograms(test_folder)\n",
    "create_spectrograms(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Première fois:\n",
      "Temps total pour load 2000 signaux: 7.638150982558727\n",
      "Temps total pour en calculer les spectrogrammes: 9.375799618661404\n",
      "Comparé au temps pour charger les spectrogrammes: 9.05385460332036\n",
      "\n",
      "Deuxième fois:\n",
      "Temps total pour load 2000 signaux: 0.47687792032957077\n",
      "Temps total pour en calculer les spectrogrammes: 3.56374404206872\n",
      "Comparé au temps pour charger les spectrogrammes: 1.2208664305508137\n"
     ]
    }
   ],
   "source": [
    "def compute_time(folder):\n",
    "    time_load_signal = 0\n",
    "    time_load_spec = 0\n",
    "    time_spec = 0\n",
    "    start = time.perf_counter()\n",
    "    for i in range(datasets_sizes[folder]):\n",
    "        fe,signal = wavfile.read(get_path(folder,i) / \"voice.wav\")\n",
    "        time_load_signal += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        path_spec: Path = get_path(folder,i) / \"voice_spectrogram.pt\"\n",
    "        if path_spec.exists():\n",
    "            _ = torch.load(path_spec)\n",
    "        time_load_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        _ = get_spectrogram(signal)\n",
    "        time_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "\n",
    "    print(f\"Temps total pour load {datasets_sizes[folder]} signaux: {time_load_signal}\")\n",
    "    print(f\"Temps total pour en calculer les spectrogrammes: {time_spec}\")\n",
    "    print(f\"Comparé au temps pour charger les spectrogrammes: {time_load_spec}\")\n",
    "\n",
    "print(\"Première fois:\")\n",
    "compute_time(test_folder)\n",
    "print(\"\\nDeuxième fois:\")\n",
    "compute_time(test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que charger les spectrogrammes pré-calculés est plus rapide que de les calculer à chaque fois. On note aussi une grosse différence entre la première fois qu'un fichier est chargé et la seconde. Nous pensons que le système place les derniers fichiers chargés dans le cache (recharger le notebook n'y change rien, donc la différence n'apparait que la toute première fois)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "            folder: Path,\n",
    "            load_signals=True,\n",
    "            load_spectrograms=False):\n",
    "        self.folder = folder\n",
    "        self.load_signals = load_signals\n",
    "        self.load_spectrograms = load_spectrograms\n",
    "            \n",
    "    def __len__(self):\n",
    "        return datasets_sizes[self.folder]\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        d = load_signal_folder(\n",
    "            get_path(self.folder,i),\n",
    "            load_signals=self.load_signals,\n",
    "            load_spectrograms=self.load_spectrograms)\n",
    "        ret = []\n",
    "        for name in [\"voice\",\"noise\",\"mix\"]:\n",
    "            if self.load_signals:\n",
    "                ret.append(d[name][\"signal\"])\n",
    "            if self.load_spectrograms:\n",
    "                ret.append(d[name][\"spectrogram\"])\n",
    "        ret.append(d[\"SNR\"])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Dataset peut contenir les signaux et/ou les spectrogrammes. De sorte à ne charger que le nécessaire. Exemple si on veut tout charger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_folder,load_signals=True,load_spectrograms=True)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "print(\"Dataset length:\",len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal's shape:  torch.Size([32, 80000])\n",
      "Spectrogram's shape:  torch.Size([32, 257, 268])\n",
      "SNRs : ('2', '-1', '4', '4', '2', '0', '0', '1', '2', '0', '-4', '1', '0', '2', '-4', '2', '4', '0', '1', '-2', '2', '-3', '1', '0', '-1', '1', '-4', '-4', '1', '4', '-2', '1')\n"
     ]
    }
   ],
   "source": [
    "for voice_signal,voice_spec,noise_signal,noise_spec,mix_signal,mix_spec,snr in train_dataloader:\n",
    "    print(\"Signal's shape: \",voice_signal.shape)\n",
    "    print(\"Spectrogram's shape: \",voice_spec.shape)\n",
    "    print(\"SNRs :\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv-tasnet: surpassing ideal time–frequency magnitude masking for speech separation.\n",
    "(Inspired from Luo et al., 2019 and from this popular Github repository : https://github.com/JusperLee/Conv-TasNet/tree/master/Conv_TasNet_Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(\n",
    "    train_folder,\n",
    "    load_signals=True,\n",
    "    load_spectrograms=False)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "print(\"Dataset length:\",len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spec's shape: torch.Size([32, 80000])\n",
      "SNRs: ('4', '-4', '-3', '0', '-2', '-2', '-4', '-3', '0', '0', '-4', '3', '0', '1', '0', '-2', '-2', '2', '-4', '-3', '-1', '4', '3', '3', '2', '-3', '1', '-3', '-3', '0', '4', '4')\n"
     ]
    }
   ],
   "source": [
    "for voice_signal,noise_signal,mix_signal,snr in train_dataloader:\n",
    "    print(\"Spec's shape:\",voice_spec.shape)\n",
    "    print(\"SNRs:\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_param(model):\n",
    "    return sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTasNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_sources=2, \n",
    "                 num_filters=512, \n",
    "                 kernel_size=16, \n",
    "                 stride=8, \n",
    "                 bottleneck_channels=128, \n",
    "                 num_blocks=8, \n",
    "                 num_repeats=3):\n",
    "        super(ConvTasNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Conv1d(32, num_filters, kernel_size, stride, padding=kernel_size//2, bias=False)\n",
    "\n",
    "        # Separator\n",
    "        self.separator = TemporalConvNet(num_filters, \n",
    "                                         bottleneck_channels, \n",
    "                                         num_blocks, \n",
    "                                         num_repeats, \n",
    "                                         num_sources)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.ConvTranspose1d(num_filters, 1, kernel_size, stride, padding=kernel_size//2, bias=False)\n",
    "\n",
    "    def forward(self, mixture, print_shapes=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mixture (torch.Tensor): Mixture audio signal, shape (batch, 1, time).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Separated audio signals, shape (batch, num_sources, time).\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        encoded = nn.functional.relu(self.encoder(mixture))\n",
    "\n",
    "        # Separator\n",
    "        masks = self.separator(encoded)\n",
    "        if print_shapes: print(masks.shape)\n",
    "\n",
    "        # Apply masks\n",
    "        sources = masks * encoded.unsqueeze(0)\n",
    "\n",
    "        # Decoder\n",
    "        decoded_sources = torch.cat([\n",
    "            self.decoder(sources[:, i]).unsqueeze(1)\n",
    "            for i in range(sources.size(1))\n",
    "        ], dim=1)\n",
    "\n",
    "        if print_shapes: print(decoded_sources.shape)\n",
    "\n",
    "        return masks, decoded_sources\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_filters, \n",
    "                 bottleneck_channels, \n",
    "                 num_blocks, \n",
    "                 num_repeats, \n",
    "                 num_sources):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for r in range(num_repeats):\n",
    "            for b in range(num_blocks):\n",
    "                dilation = 2 ** b\n",
    "                self.layers.append(TemporalBlock(num_filters, bottleneck_channels, dilation))\n",
    "\n",
    "        self.mask_generator = nn.Conv1d(num_filters, num_filters * num_sources, kernel_size=1)\n",
    "        self.num_sources = num_sources\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "    def forward(self, x, print_shapes=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Encoded mixture, shape (batch, num_filters, time).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Masks for each source, shape (batch, num_sources, num_filters, time).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if print_shapes: print(x.shape)\n",
    "\n",
    "        if print_shapes: print(\"-\")\n",
    "        masks = self.mask_generator(x)  # shape (batch, num_filters * num_sources, time)\n",
    "        #masks = masks.view(masks.size(0), self.num_sources, self.num_filters, -1)  # shape (batch, num_sources, num_filters, time)\n",
    "        masks = masks.view(1, self.num_sources, self.num_filters, masks.size(1))  # shape (batch, num_sources, num_filters, time)\n",
    "        masks = nn.functional.softmax(masks, dim=1)  # Normalize across sources\n",
    "        if print_shapes: print(masks.shape)\n",
    "\n",
    "        return masks\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, num_filters, bottleneck_channels, dilation):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(num_filters)\n",
    "        self.conv1x1 = nn.Conv1d(num_filters, bottleneck_channels, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.depthwise_conv = nn.Conv1d(\n",
    "            bottleneck_channels, \n",
    "            bottleneck_channels, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=dilation, \n",
    "            dilation=dilation, \n",
    "            groups=bottleneck_channels, \n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.pointwise_conv = nn.Conv1d(bottleneck_channels, num_filters, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, print_shapes=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor, shape (batch, num_filters, time).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor, shape (batch, num_filters, time).\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.relu(self.conv1x1(x))\n",
    "        x = self.relu(self.depthwise_conv(x))\n",
    "        x = self.pointwise_conv(x)\n",
    "        \n",
    "        if print_shapes: print(x.shape)\n",
    "                \n",
    "        return x + residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvTasNet(\n",
      "  (encoder): Conv1d(32, 512, kernel_size=(16,), stride=(8,), padding=(8,), bias=False)\n",
      "  (separator): TemporalConvNet(\n",
      "    (layers): ModuleList(\n",
      "      (0): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (7): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (8): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (9): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (10): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (11): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (12): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (13): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (14): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (15): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (16): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (17): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (18): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (19): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (20): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (21): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (22): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (23): TemporalBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv1x1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (depthwise_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=128, bias=False)\n",
      "        (pointwise_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (mask_generator): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (decoder): ConvTranspose1d(512, 1, kernel_size=(16,), stride=(8,), padding=(8,), bias=False)\n",
      ")\n",
      "nombre de paramètres: 3975168\n",
      "torch.Size([1, 2, 512, 10001])\n",
      "torch.Size([1, 2, 1, 80000])\n",
      "torch.Size([1, 2, 512, 10001])\n",
      "torch.Size([1, 2, 1, 80000])\n"
     ]
    }
   ],
   "source": [
    "model = ConvTasNet()\n",
    "print(model)\n",
    "print(\"nombre de paramètres:\", count_n_param(model))\n",
    "\n",
    "mask,output = model(voice_signal,print_shapes=True)\n",
    "mask, output = model(mix_signal,print_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        dataloader (DataLoader): DataLoader for the validation/test dataset.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to use for computation.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average loss over the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Load data\n",
    "            voice_signal, noise_signal, mix_signal, snr = batch\n",
    "            \n",
    "            # Convert mix_signal to mono-channel if needed\n",
    "            mixture = mix_signal.unsqueeze(1).to(device)  # Shape: [batch, 1, time]\n",
    "            \n",
    "            # Stack voice and noise signals as sources\n",
    "            sources = torch.stack([voice_signal, noise_signal], dim=1).to(device)  # Shape: [batch, num_sources, time]\n",
    "            \n",
    "            # Forward pass\n",
    "            predicted_sources = model(mixture)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_sources, sources)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataloader\n",
    "test_dataset = MyDataset(test_folder, load_signals=False, load_spectrograms=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/157 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv1d(): argument 'dilation' must be tuple of ints, but found element of type int at pos 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m voice_signal, noise_signal, mix_signal \u001b[38;5;241m=\u001b[39m voice_signal\u001b[38;5;241m.\u001b[39mto(device), noise_signal\u001b[38;5;241m.\u001b[39mto(device), mix_signal\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m mask, output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmix_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m pred_voice_spec \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m*\u001b[39m mix_signal\n\u001b[1;32m     41\u001b[0m pred_noise_spec \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mmask) \u001b[38;5;241m*\u001b[39m mix_signal\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[160], line 37\u001b[0m, in \u001b[0;36mConvTasNet.forward\u001b[0;34m(self, mixture, print_shapes)\u001b[0m\n\u001b[1;32m     34\u001b[0m encoded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(mixture))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Separator\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_shapes: \u001b[38;5;28mprint\u001b[39m(masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Apply masks\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[160], line 82\u001b[0m, in \u001b[0;36mTemporalConvNet.forward\u001b[0;34m(self, x, print_shapes)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    x (torch.Tensor): Encoded mixture, shape (batch, num_filters, time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Masks for each source, shape (batch, num_sources, num_filters, time).\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 82\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_shapes: \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_shapes: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[160], line 129\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x, print_shapes)\u001b[0m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1x1(x))\n\u001b[0;32m--> 129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepthwise_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    130\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointwise_conv(x)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_shapes: \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv1d(): argument 'dilation' must be tuple of ints, but found element of type int at pos 1"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_sources = 2\n",
    "N = 256\n",
    "L = 20\n",
    "B = 256\n",
    "H = 512\n",
    "P = 3\n",
    "X = 8\n",
    "R = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ConvTasNet(num_sources=num_sources, num_filters=N, kernel_size=L, stride=X, bottleneck_channels=H, num_blocks=B, num_repeats=R)\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model.to(device) # Move model to the GPU\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for batch in progress_bar:\n",
    "        # Load data\n",
    "        voice_signal, noise_signal, mix_signal, snr = batch\n",
    "        voice_signal, noise_signal, mix_signal = voice_signal.to(device), noise_signal.to(device), mix_signal.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        mask, output = model(mix_signal)\n",
    "        pred_voice_spec = mask * mix_signal\n",
    "        pred_noise_spec = (1-mask) * mix_signal\n",
    "\n",
    "        # Compute loss\n",
    "        loss = (loss_fn(pred_voice_spec,voice_spec)\n",
    "                + loss_fn(pred_noise_spec,noise_spec))\n",
    "            \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    # Save the train loss for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss / len(train_dataloader):.4f}\")\n",
    "    train_losses.append(epoch_loss / len(train_dataloader))\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss = evaluate_model(model, test_dataloader, criterion, device)\n",
    "    validation_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"conv_tasnet.pth\")\n",
    "print(\"Training complete! Model saved as 'conv_tasnet.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracé de la loss au cours des epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
