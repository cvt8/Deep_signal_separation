{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.axes._axes import Axes\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import stft,istft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=6><b>Meta paramètres</b></font>\n",
    "\n",
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute puis les sauvegarder sur le disque. Toutefois, <font color=\"red\"><b>ceci triple l'espace occupé sur disque</b></font>: passant de 6.3 Go à 17.2 Go. Veuillez donc préciser le paramètre `SAVE_SPECTROGRAMS` selon si vous pouvez utiliser cet espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = Path(\"source_separation\")\n",
    "SAVE_SPECTROGRAMS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des signaux, Visualisation et Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des signaux et spectrogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = DIRECTORY / \"train\"\n",
    "train_small_folder = DIRECTORY / \"train_small\"\n",
    "test_folder = DIRECTORY / \"test\"\n",
    "get_path = lambda folder,i : folder / (\"000\"+str(i))[-4:]\n",
    "\n",
    "datasets_sizes = {\n",
    "    train_small_folder : 50,\n",
    "    train_folder : 5000,\n",
    "    test_folder : 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les signaux ont la même fréquence d'échantillonage, même longueur; et donc les mêmes f et t échantillonés pour le Spectrogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_same(folder=train_small_folder):\n",
    "    f_ref = None\n",
    "    t_ref = None\n",
    "    for i in range(datasets_sizes[train_small_folder]):\n",
    "        fe,signal = wavfile.read(get_path(train_small_folder,i) / \"voice.wav\")\n",
    "        len_signal = len(signal)\n",
    "        f_spec,t_spec,spec = stft(\n",
    "            signal,fs=fe,\n",
    "            nperseg=400,nfft=512,noverlap=100)\n",
    "        if f_ref is None: f_ref = f_spec ; t_ref = t_spec\n",
    "        assert np.allclose(f_spec,f_ref)\n",
    "        assert np.allclose(t_spec,t_ref)\n",
    "    return fe,f_ref,t_ref\n",
    "\n",
    "fe,f_ref,t_ref = check_all_same()\n",
    "get_spectrogram = lambda signal : stft(signal,fs=fe,nperseg=400,nfft=512,noverlap=100)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal_folder(folder: Path,\n",
    "        load_signals=True,\n",
    "        load_spectrograms=True) -> dict[str,dict]:\n",
    "    \"\"\"\n",
    "    Return a dictionary with 3 sub dicts: \"voice\", \"noise\" and \"mix\"; and an \"SNR\" key.\n",
    "    Each sub dict has 3 keys: \"filename\", \"signal\" and \"spectrogram\" \n",
    "    (except if load_signals or load_spectrograms are set to False)\n",
    "    \"\"\"\n",
    "    keys = [\"voice\",\"noise\",\"mix\"]\n",
    "    res = dict((k,dict()) for k in keys)\n",
    "    for f in folder.iterdir():\n",
    "        assert f.is_file()\n",
    "        if \"voice\" in f.name: key = \"voice\"\n",
    "        elif \"noise\" in f.name: key = \"noise\"\n",
    "        else: \n",
    "            key = \"mix\"\n",
    "            if f.suffix == \".wav\":\n",
    "                res[\"SNR\"] = f.name.removesuffix(\".wav\").split(\"_\")[-1]\n",
    "        if f.suffix == \".wav\" and load_signals:\n",
    "            fe,signal = wavfile.read(f)\n",
    "            res[key][\"filename\"] = f.name\n",
    "            res[key][\"signal\"] = signal\n",
    "        elif f.suffix == \".pt\" and SAVE_SPECTROGRAMS and load_spectrograms: \n",
    "            # when SAVE_SPECTROGRAMS is False, we shouldn't be able to load them \n",
    "            # to save time, otherwise it's cheating.\n",
    "            res[key][\"spectrogram\"] = torch.load(f,weights_only=True)\n",
    "    # Create missing spectrograms\n",
    "    if load_spectrograms:\n",
    "        for key in keys:\n",
    "            if \"spectrogram\" not in res[key]:\n",
    "                assert load_signals\n",
    "                spec = get_spectrogram(res[key][\"signal\"])\n",
    "                res[key][\"spectrogram\"] = spec\n",
    "                if SAVE_SPECTROGRAMS:\n",
    "                    torch.save(torch.tensor(spec),folder / f\"{key}_spectrogram.pt\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_all_spectrograms():\n",
    "    for folder in [train_folder,train_small_folder,test_folder]:\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            folder_i: Path = get_path(folder,i)\n",
    "            for f in folder_i.iterdir():\n",
    "                if \"spectrogram\" in f.name:\n",
    "                    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute, en sauvegardant tous les spectrogrammes sur disque. Attention, <font color=\"red\"><b>ceci triple l'espace occupé sur disque</b></font>: passant de 6.3 Go à 17.2 Go.\n",
    "Pour les retirer utiliser: `remove_all_spectrograms()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrograms(folder: Path):\n",
    "    if SAVE_SPECTROGRAMS and not (folder/\"0000\"/\"voice_spectrogram.pt\").exists():\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            load_signal_folder(get_path(folder,i))\n",
    "\n",
    "create_spectrograms(train_small_folder)\n",
    "create_spectrograms(test_folder)\n",
    "create_spectrograms(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Première fois:\n",
      "Temps total pour load 2000 signaux: 36.739902939647436\n",
      "Temps total pour en calculer les spectrogrammes: 5.3411676697432995\n",
      "Comparé au temps pour charger les spectrogrammes: 29.36331645771861\n",
      "\n",
      "Deuxième fois:\n",
      "Temps total pour load 2000 signaux: 1.0172409191727638\n",
      "Temps total pour en calculer les spectrogrammes: 2.5520561523735523\n",
      "Comparé au temps pour charger les spectrogrammes: 2.1508601531386375\n"
     ]
    }
   ],
   "source": [
    "def compute_time(folder):\n",
    "    time_load_signal = 0\n",
    "    time_load_spec = 0\n",
    "    time_spec = 0\n",
    "    start = time.perf_counter()\n",
    "    for i in range(datasets_sizes[folder]):\n",
    "        fe,signal = wavfile.read(get_path(folder,i) / \"voice.wav\")\n",
    "        time_load_signal += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        path_spec: Path = get_path(folder,i) / \"voice_spectrogram.pt\"\n",
    "        if path_spec.exists():\n",
    "            _ = torch.load(path_spec,weights_only=True)\n",
    "        time_load_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        _ = get_spectrogram(signal)\n",
    "        time_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "\n",
    "    print(f\"Temps total pour load {datasets_sizes[folder]} signaux: {time_load_signal}\")\n",
    "    print(f\"Temps total pour en calculer les spectrogrammes: {time_spec}\")\n",
    "    print(f\"Comparé au temps pour charger les spectrogrammes: {time_load_spec}\")\n",
    "\n",
    "print(\"Première fois:\")\n",
    "compute_time(test_folder)\n",
    "print(\"\\nDeuxième fois:\")\n",
    "compute_time(test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que charger les spectrogrammes pré-calculés est plus rapide que de les calculer à chaque fois. On note aussi une grosse différence entre la première fois qu'un fichier est chargé et la seconde. Nous pensons que le système place les derniers fichiers chargés dans le cache (recharger le notebook n'y change rien, donc la différence n'apparait que la toute première fois)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "            folder: Path,\n",
    "            load_signals=True,\n",
    "            load_spectrograms=False):\n",
    "        self.folder = folder\n",
    "        self.load_signals = load_signals\n",
    "        self.load_spectrograms = load_spectrograms\n",
    "            \n",
    "    def __len__(self):\n",
    "        return datasets_sizes[self.folder]\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        d = load_signal_folder(\n",
    "            get_path(self.folder,i),\n",
    "            load_signals=self.load_signals,\n",
    "            load_spectrograms=self.load_spectrograms)\n",
    "        ret = []\n",
    "        for name in [\"voice\",\"noise\",\"mix\"]:\n",
    "            if self.load_signals:\n",
    "                ret.append(d[name][\"signal\"])\n",
    "            if self.load_spectrograms:\n",
    "                ret.append(d[name][\"spectrogram\"])\n",
    "        ret.append(d[\"SNR\"])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Dataset peut contenir les signaux et/ou les spectrogrammes. De sorte à ne charger que le nécessaire. Exemple si on veut tout charger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal's shape:  torch.Size([32, 80000])\n",
      "Spectrogram's shape:  torch.Size([32, 257, 268])\n",
      "SNRs : ('-1', '4', '0', '0', '-2', '2', '-2', '4', '0', '-2', '0', '2', '4', '3', '-2', '-4', '3', '3', '3', '-2', '-3', '0', '0', '2', '2', '3', '3', '3', '3', '1', '2', '0')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_folder,load_signals=True,load_spectrograms=True)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "print(\"Dataset length:\",len(train_dataset))\n",
    "for voice_signal,voice_spec,noise_signal,noise_spec,mix_signal,mix_spec,snr in train_dataloader:\n",
    "    print(\"Signal's shape: \",voice_signal.shape)\n",
    "    print(\"Spectrogram's shape: \",voice_spec.shape)\n",
    "    print(\"SNRs :\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv-tasnet: surpassing ideal time–frequency magnitude masking for speech separation.\n",
    "(Inspired from Luo et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(\n",
    "    train_folder,\n",
    "    load_signals=False,\n",
    "    load_spectrograms=True)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spec's shape: torch.Size([32, 257, 268])\n",
      "SNRs: ('-2', '-1', '-2', '-1', '1', '1', '1', '4', '2', '4', '3', '-1', '3', '1', '4', '-2', '4', '2', '0', '1', '4', '-1', '3', '2', '0', '4', '0', '-3', '-4', '-1', '0', '-1')\n"
     ]
    }
   ],
   "source": [
    "for voice_spec,noise_spec,mix_spec,snr in train_dataloader:\n",
    "    print(\"Spec's shape:\",voice_spec.shape)\n",
    "    print(\"SNRs:\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTasNet(nn.Module):\n",
    "    def __init__(self, N=256, L=20, B=256, H=512, P=3, X=8, R=4):\n",
    "        super(ConvTasNet, self).__init__()\n",
    "        self.encoder = nn.Conv1d(1, N, L, stride=L//2, bias=False)\n",
    "        self.separator = nn.Sequential(\n",
    "            nn.Conv1d(N, B, 1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(B, B, 1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(B, H, 1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(H, H, P, groups=H, padding=P//2),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(H, B, 1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(B, N, 1)\n",
    "        )\n",
    "        self.decoder = nn.ConvTranspose1d(N, 1, L, stride=L//2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.separator(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._dynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m ConvTasNet(N, L, B, H, P, X, R)\n\u001b[1;32m     14\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n\u001b[0;32m---> 15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/optim/adam.py:78\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m,\n\u001b[1;32m     75\u001b[0m              weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, amsgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, foreach: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m              maximize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, capturable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m lr:\n\u001b[0;32m---> 78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lr))\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eps:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid epsilon value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(eps))\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/optim/optimizer.py:371\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/_compile.py:27\u001b[0m, in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._dynamo'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "N = 256\n",
    "L = 20\n",
    "B = 256\n",
    "H = 512\n",
    "P = 3\n",
    "X = 8\n",
    "R = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ConvTasNet(N, L, B, H, P, X, R)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (voice_signal, noise_signal, mix_signal, snr) in enumerate(train_dataloader):\n",
    "        mix_signal = mix_signal.unsqueeze(1)  # Add channel dimension\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mix_signal)\n",
    "        loss = criterion(output, voice_signal)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[256, 1], expected input with shape [*, 256, 1], but got input of size[4, 3199, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Sample input to verify architecture\u001b[39;00m\n\u001b[1;32m     63\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32000\u001b[39m)  \u001b[38;5;66;03m# [batch, channels, time]\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m sample_output \u001b[38;5;241m=\u001b[39m \u001b[43msample_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Confirm output shape matches input shape\u001b[39;00m\n\u001b[1;32m     67\u001b[0m sample_input\u001b[38;5;241m.\u001b[39mshape, sample_output\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_children\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]]:\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over immediate children modules, yielding both\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;124;03m    the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m        (string, Module): Tuple containing a name and child module\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m \n\u001b[1;32m   1735\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m \n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m        >>> for name, module in model.named_children():\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m        >>>     if name in ['conv4', 'conv5']:\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;124;03m        >>>         print(module)\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mRefinedConvTasNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)  \u001b[38;5;66;03m# [batch, N, time]\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, N, time]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     separated \u001b[38;5;241m=\u001b[39m encoded \u001b[38;5;241m*\u001b[39m masks  \u001b[38;5;66;03m# Apply masks\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(separated)  \u001b[38;5;66;03m# [batch, 1, time]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_children\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]]:\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over immediate children modules, yielding both\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;124;03m    the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m        (string, Module): Tuple containing a name and child module\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m \n\u001b[1;32m   1735\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m \n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m        >>> for name, module in model.named_children():\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m        >>>     if name in ['conv4', 'conv5']:\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;124;03m        >>>         print(module)\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m, in \u001b[0;36mTemporalConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_children\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]]:\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over immediate children modules, yielding both\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;124;03m    the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m        (string, Module): Tuple containing a name and child module\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m \n\u001b[1;32m   1735\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m \n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m        >>> for name, module in model.named_children():\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m        >>>     if name in ['conv4', 'conv5']:\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;124;03m        >>>         print(module)\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: Module) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModuleList\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Appends a given module to the end of the list.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m        module (nn.Module): module to append\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_module(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)), module)\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_children\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]]:\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over immediate children modules, yielding both\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;124;03m    the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m        (string, Module): Tuple containing a name and child module\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m \n\u001b[1;32m   1735\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m \n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m        >>> for name, module in model.named_children():\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m        >>>     if name in ['conv4', 'conv5']:\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;124;03m        >>>         print(module)\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "Cell \u001b[0;32mIn[18], line 53\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Layer normalization\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepthwise_conv(x)\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointwise_conv(x)\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_children\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]]:\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over immediate children modules, yielding both\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;124;03m    the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m        (string, Module): Tuple containing a name and child module\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m \n\u001b[1;32m   1735\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m \n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m        >>> for name, module in model.named_children():\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m        >>>     if name in ['conv4', 'conv5']:\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;124;03m        >>>         print(module)\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGroupNorm\u001b[39;00m(Module):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies Group Normalization over a mini-batch of inputs as described in\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    the paper `Group Normalization <https://arxiv.org/abs/1803.08494>`__\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    .. math::\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    The input channels are separated into :attr:`num_groups` groups, each containing\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    ``num_channels / num_groups`` channels. :attr:`num_channels` must be divisible by\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    :attr:`num_groups`. The mean and standard-deviation are calculated\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    separately over the each group. :math:`\\gamma` and :math:`\\beta` are learnable\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    per-channel affine transform parameter vectors of size :attr:`num_channels` if\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    :attr:`affine` is ``True``.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    The standard-deviation is calculated via the biased estimator, equivalent to\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    `torch.var(input, unbiased=False)`.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    This layer uses statistics computed from input data in both training and\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    evaluation modes.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;124;03m        num_groups (int): number of groups to separate the channels into\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m        num_channels (int): number of channels expected in input\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m        eps: a value added to the denominator for numerical stability. Default: 1e-5\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m        affine: a boolean value that when set to ``True``, this module\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m            has learnable per-channel affine parameters initialized to ones (for weights)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m            and zeros (for biases). Default: ``True``.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    Shape:\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m        - Input: :math:`(N, C, *)` where :math:`C=\\text{num\\_channels}`\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m        - Output: :math:`(N, C, *)` (same shape as input)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m        >>> input = torch.randn(20, 6, 10, 10)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m        >>> # Separate 6 channels into 3 groups\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        >>> m = nn.GroupNorm(3, 6)\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m        >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m        >>> m = nn.GroupNorm(6, 6)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m        >>> # Put all 6 channels into a single group (equivalent with LayerNorm)\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m        >>> m = nn.GroupNorm(1, 6)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m        >>> # Activating the module\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m        >>> output = m(input)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     __constants__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_groups\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_channels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffine\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    241\u001b[0m     num_groups: \u001b[38;5;28mint\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   2890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2891\u001b[0m         kl_div,\n\u001b[1;32m   2892\u001b[0m         (\u001b[38;5;28minput\u001b[39m, target),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         log_target\u001b[38;5;241m=\u001b[39mlog_target,\n\u001b[1;32m   2899\u001b[0m     )\n\u001b[0;32m-> 2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2901\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_enum(size_average, reduce)\n\u001b[1;32m   2902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[256, 1], expected input with shape [*, 256, 1], but got input of size[4, 3199, 256]"
     ]
    }
   ],
   "source": [
    "class RefinedConvTasNet(nn.Module):\n",
    "    def __init__(self, N=256, L=20, B=256, H=512, P=3, X=8, R=4):\n",
    "        \"\"\"\n",
    "        Conv-TasNet implementation based on the paper.\n",
    "        N: Number of filters in the encoder/decoder.\n",
    "        L: Length of filters in samples.\n",
    "        B: Number of channels in bottleneck and the residual paths.\n",
    "        H: Number of channels in convolutional blocks.\n",
    "        P: Kernel size in convolutional blocks.\n",
    "        X: Number of convolutional blocks in each repeat.\n",
    "        R: Number of repeats of the TCN.\n",
    "        \"\"\"\n",
    "        super(RefinedConvTasNet, self).__init__()\n",
    "        self.encoder = nn.Conv1d(1, N, kernel_size=L, stride=L // 2, bias=False)\n",
    "        self.separator = TemporalConvNet(N, B, H, P, X, R)\n",
    "        self.decoder = nn.ConvTranspose1d(N, 1, kernel_size=L, stride=L // 2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)  # [batch, N, time]\n",
    "        masks = self.separator(encoded)  # [batch, N, time]\n",
    "        separated = encoded * masks  # Apply masks\n",
    "        output = self.decoder(separated)  # [batch, 1, time]\n",
    "        return output\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, N, B, H, P, X, R):\n",
    "        \"\"\"\n",
    "        Temporal Convolutional Network (TCN) for separator.\n",
    "        \"\"\"\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        for r in range(R):\n",
    "            for x in range(X):\n",
    "                dilation = 2 ** x  # Exponential dilation\n",
    "                layers.append(ConvBlock(N, B, H, P, dilation))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, N, B, H, P, dilation):\n",
    "        \"\"\"\n",
    "        Single convolutional block in the TCN.\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm([N, 1])\n",
    "        self.depthwise_conv = nn.Conv1d(B, B, kernel_size=P, dilation=dilation, padding=dilation, groups=B)\n",
    "        self.pointwise_conv = nn.Conv1d(B, N, kernel_size=1)\n",
    "        self.activation = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x.transpose(1, 2)).transpose(1, 2)  # Layer normalization\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "# Hyperparameters for testing\n",
    "N, L, B, H, P, X, R = 256, 20, 256, 512, 3, 8, 4\n",
    "sample_model = RefinedConvTasNet(N, L, B, H, P, X, R)\n",
    "\n",
    "# Sample input to verify architecture\n",
    "sample_input = torch.randn(4, 1, 32000)  # [batch, channels, time]\n",
    "sample_output = sample_model(sample_input)\n",
    "\n",
    "# Confirm output shape matches input shape\n",
    "sample_input.shape, sample_output.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
