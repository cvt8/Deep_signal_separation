{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.axes._axes import Axes\n",
    "\n",
    "from IPython.display import Audio\n",
    "# Audio(data=signal.T,rate=fe)\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import stft,istft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=6><b>Meta paramètres</b></font>\n",
    "\n",
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute puis les sauvegarder sur le disque. Toutefois, <font color=\"red\"><b>ceci triple l'espace occupé sur disque</b></font>: passant de 6.3 Go à 17.2 Go. Veuillez donc préciser le paramètre `SAVE_SPECTROGRAMS` selon si vous pouvez utiliser cet espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = Path(\"source_separation\")\n",
    "SAVE_SPECTROGRAMS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des signaux, Visualisation et Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des signaux et spectrogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = DIRECTORY / \"train\"\n",
    "train_small_folder = DIRECTORY / \"train_small\"\n",
    "test_folder = DIRECTORY / \"test\"\n",
    "get_path = lambda folder,i : folder / (\"000\"+str(i))[-4:]\n",
    "\n",
    "datasets_sizes = {\n",
    "    train_small_folder : 50,\n",
    "    train_folder : 5000,\n",
    "    test_folder : 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les signaux ont la même fréquence d'échantillonage, même longueur; et donc les mêmes f et t échantillonés pour le Spectrogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram shape: (257, 320)\n",
      "Original signal (80000,)\n",
      "Reconstructed signal (80000,)\n",
      "Allclose True\n",
      "(320,)\n"
     ]
    }
   ],
   "source": [
    "stft_kwargs = {\n",
    "    \"nperseg\":400,\n",
    "    \"nfft\":512,\n",
    "    \"noverlap\":149 #86,\n",
    "}\n",
    "def check_all_same(folder=train_small_folder):\n",
    "    f_ref = None\n",
    "    t_ref = None\n",
    "    for i in range(datasets_sizes[train_small_folder]):\n",
    "        fe,signal = wavfile.read(get_path(train_small_folder,i) / \"voice.wav\")\n",
    "        len_signal = len(signal)\n",
    "        f_spec,t_spec,spec = stft(\n",
    "            signal,fs=fe,**stft_kwargs)\n",
    "        if f_ref is None: f_ref = f_spec ; t_ref = t_spec\n",
    "        assert np.allclose(f_spec,f_ref)\n",
    "        assert np.allclose(t_spec,t_ref)\n",
    "    return fe,f_ref,t_ref\n",
    "\n",
    "fe,f_ref,t_ref = check_all_same()\n",
    "get_spectrogram = lambda signal : stft(signal,fs=fe,**stft_kwargs)[2]\n",
    "reverse_spectrogram = lambda spec : istft(spec,fe,**stft_kwargs)[1][:80000]\n",
    "\n",
    "# Pour vérifier que le spectrogramme est bien inversible:\n",
    "def test():\n",
    "    i = np.random.randint(0,datasets_sizes[train_folder])\n",
    "    signal = wavfile.read(get_path(train_folder,i)/\"voice.wav\")[1]\n",
    "    spec = get_spectrogram(signal)\n",
    "    resignal = reverse_spectrogram(spec)\n",
    "    print(\"Spectrogram shape:\",spec.shape)\n",
    "    print(\"Original signal\",signal.shape)\n",
    "    print(\"Reconstructed signal\",resignal.shape)\n",
    "    print(\"Allclose\",np.allclose(signal,resignal))\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_load(file):\n",
    "    # depending on torch.__version__ you may need a 'weights_only=True' argument\n",
    "    # to avoid a warning (and in old versions this kwarg doesn't exist)\n",
    "    # I'm not sure since which version this warning exists,\n",
    "    # so I'm doing a try/except instead of a if version>2.1: ...\n",
    "    try:\n",
    "        return torch.load(file,weights_only=True)\n",
    "    except:\n",
    "        return torch.load(file)\n",
    "\n",
    "def load_signal_folder(folder: Path,\n",
    "        load_signals=True,\n",
    "        load_spectrograms=True) -> dict[str,dict]:\n",
    "    \"\"\"\n",
    "    Return a dictionary with 3 sub dicts: \"voice\", \"noise\" and \"mix\"; and an \"SNR\" key.\n",
    "    Each sub dict has 3 keys: \"filename\", \"signal\" and \"spectrogram\" \n",
    "    (except if load_signals or load_spectrograms are set to False)\n",
    "    \"\"\"\n",
    "    keys = [\"voice\",\"noise\",\"mix\"]\n",
    "    res = dict((k,dict()) for k in keys)\n",
    "    for f in folder.iterdir():\n",
    "        assert f.is_file()\n",
    "        if \"voice\" in f.name: key = \"voice\"\n",
    "        elif \"noise\" in f.name: key = \"noise\"\n",
    "        else: \n",
    "            key = \"mix\"\n",
    "            if f.suffix == \".wav\":\n",
    "                res[\"SNR\"] = f.name.removesuffix(\".wav\").split(\"_\")[-1]\n",
    "        if f.suffix == \".wav\" and load_signals:\n",
    "            fe,signal = wavfile.read(f)\n",
    "            res[key][\"filename\"] = f.name\n",
    "            res[key][\"signal\"] = signal\n",
    "        elif f.suffix == \".pt\" and SAVE_SPECTROGRAMS and load_spectrograms: \n",
    "            # when SAVE_SPECTROGRAMS is False, we shouldn't be able to load them \n",
    "            # to save time, otherwise it's cheating.\n",
    "            res[key][\"spectrogram\"] = torch_load(f)\n",
    "    # Create missing spectrograms\n",
    "    if load_spectrograms:\n",
    "        for key in keys:\n",
    "            if \"spectrogram\" not in res[key]:\n",
    "                assert load_signals\n",
    "                spec_abs = abs(get_spectrogram(res[key][\"signal\"]))\n",
    "                res[key][\"spectrogram\"] = spec_abs\n",
    "                if SAVE_SPECTROGRAMS:\n",
    "                    torch.save(torch.tensor(spec_abs),folder / f\"{key}_spectrogram.pt\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_all_spectrograms():\n",
    "    for folder in [train_folder,train_small_folder,test_folder]:\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            folder_i: Path = get_path(folder,i)\n",
    "            for f in folder_i.iterdir():\n",
    "                if \"spectrogram\" in f.name:\n",
    "                    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute, en sauvegardant toutes les amplitudes des spectrogrammes sur disque (puisque l'apprentissage travaille avec les amplitudes, pas besoin de sauvegarder la phase). Attention, <font color=\"red\"><b>ceci double l'espace occupé sur disque</b></font>: passant de 6.3 Go à 12.8 Go.\n",
    "Pour les retirer utiliser: `remove_all_spectrograms()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrograms(folder: Path):\n",
    "    if SAVE_SPECTROGRAMS and not (folder/\"0000\"/\"voice_spectrogram.pt\").exists():\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            load_signal_folder(get_path(folder,i))\n",
    "\n",
    "create_spectrograms(train_small_folder)\n",
    "create_spectrograms(test_folder)\n",
    "create_spectrograms(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Première fois:\n",
      "Temps total pour load 2000 signaux: 0.7348379389877664\n",
      "Temps total pour en calculer les spectrogrammes: 1.9839318960275705\n",
      "Comparé au temps pour charger les spectrogrammes: 3.389195533981365\n",
      "\n",
      "Deuxième fois:\n",
      "Temps total pour load 2000 signaux: 0.20108442701166496\n",
      "Temps total pour en calculer les spectrogrammes: 1.7352007419658548\n",
      "Comparé au temps pour charger les spectrogrammes: 0.5324180929983413\n"
     ]
    }
   ],
   "source": [
    "def compute_time(folder):\n",
    "    time_load_signal = 0\n",
    "    time_load_spec = 0\n",
    "    time_spec = 0\n",
    "    start = time.perf_counter()\n",
    "    for i in range(datasets_sizes[folder]):\n",
    "        fe,signal = wavfile.read(get_path(folder,i) / \"voice.wav\")\n",
    "        time_load_signal += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        path_spec: Path = get_path(folder,i) / \"voice_spectrogram.pt\"\n",
    "        if path_spec.exists():\n",
    "            _ = torch_load(path_spec)\n",
    "        time_load_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        _ = get_spectrogram(signal)\n",
    "        time_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "\n",
    "    print(f\"Temps total pour load {datasets_sizes[folder]} signaux: {time_load_signal}\")\n",
    "    print(f\"Temps total pour en calculer les spectrogrammes: {time_spec}\")\n",
    "    print(f\"Comparé au temps pour charger les spectrogrammes: {time_load_spec}\")\n",
    "\n",
    "print(\"Première fois:\")\n",
    "compute_time(test_folder)\n",
    "print(\"\\nDeuxième fois:\")\n",
    "compute_time(test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que charger les spectrogrammes pré-calculés est plus rapide que de les calculer à chaque fois. On note aussi une grosse différence entre la première fois qu'un fichier est chargé et la seconde, j'imagine que le système place les derniers fichiers chargés dans le cache (recharger le notebook n'y change rien, donc la différence n'apparait que la toute première fois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Note: les versions très récentes de `ipympl` ont une erreur de frappe dans le code, avec une variable nommé \"buttons\" au lieu de \"button\". Ainsi, si vous avez une version instable de `ipympl`, il se peut que d'un coup la cellule interactive ci-dessous écrivent des dizaines d'erreurs à la chaine. Nous n'avons pas trouvé ce qui les déclenche, mais vous pouvez les ignorer (puisqu'elles n'empechent pas la cellule de tourner), sinon vous pouvez simplement ouvrir le fichier d'où vient l'erreur (en cliquant sur la ligne d'erreur qui s'affiche des dizaines de fois), et changé \"buttons\", par \"button\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e891327ae842d39876be11c24c3e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='i', max=49), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_signal(folder):\n",
    "    signals = load_signal_folder(folder)\n",
    "    colorbars = []\n",
    "    for ax,name in zip(axs,[\"voice\",\"noise\",\"mix\"]):\n",
    "        spec_dB = 10*np.log10(signals[name][\"spectrogram\"].numpy())\n",
    "        plt_obj = ax.pcolormesh(\n",
    "            t_ref,f_ref,spec_dB,\n",
    "            vmax=np.percentile(spec_dB,99),\n",
    "            vmin=np.percentile(spec_dB,10))\n",
    "        colorbars.append(plt.colorbar(plt_obj,ax=ax))\n",
    "        ax.set_title(signals[name][\"filename\"])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "        colorbars[-1].ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "        ax.set_xlabel(\"Temps (ms)\",fontsize=4)\n",
    "        ax.set_ylabel(\"Fréquence (GHz)\",fontsize=4)\n",
    "    return colorbars\n",
    "\n",
    "\n",
    "folder = train_small_folder\n",
    "plt.close(\"viz\")\n",
    "fig_viz = plt.figure(\"viz\",figsize=(7,2.5))\n",
    "gs = gridspec.GridSpec(2,3) # to get good control of the color bars\n",
    "ax_slices = [np.s_[:,i] for i in range(3)]\n",
    "axs = [fig_viz.add_subplot(gs[sli]) for sli in ax_slices]\n",
    "colorbars = []\n",
    "\n",
    "@widgets.interact(i=(0,datasets_sizes[folder]-1,1))\n",
    "def update(i=1):\n",
    "    for ax in axs: ax.cla()\n",
    "    global colorbars\n",
    "    fig_viz = plt.figure(\"viz\")\n",
    "    plt.title(\"\")\n",
    "    try: \n",
    "        if colorbars != []:\n",
    "            for colorbar in colorbars:\n",
    "                fig_viz.delaxes(colorbar.ax)\n",
    "            gs = gridspec.GridSpec(2,3)\n",
    "            for ax,sli in zip(axs,ax_slices):\n",
    "                ax.set_position(gs[sli].get_position(fig_viz))\n",
    "                ax.set_subplotspec(gs[sli])\n",
    "    except:\n",
    "        print('got an error')\n",
    "        pass\n",
    "    folder_i = get_path(folder,i)\n",
    "    colorbars = visualize_signal(folder_i)\n",
    "    fig_viz.suptitle(f\"Spectogrammes pour {Path(folder_i.parent.name)/folder_i.name}\")\n",
    "    fig_viz.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "            folder: Path,\n",
    "            load_signals=True,\n",
    "            load_spectrograms=False):\n",
    "        self.folder = folder\n",
    "        self.load_signals = load_signals\n",
    "        self.load_spectrograms = load_spectrograms\n",
    "            \n",
    "    def __len__(self):\n",
    "        return datasets_sizes[self.folder]\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        d = load_signal_folder(\n",
    "            get_path(self.folder,i),\n",
    "            load_signals=self.load_signals,\n",
    "            load_spectrograms=self.load_spectrograms)\n",
    "        ret = []\n",
    "        for name in [\"voice\",\"noise\",\"mix\"]:\n",
    "            if self.load_signals:\n",
    "                ret.append(d[name][\"signal\"])\n",
    "            if self.load_spectrograms:\n",
    "                ret.append(d[name][\"spectrogram\"])\n",
    "        ret.append(d[\"SNR\"])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Dataset peut contenir les signaux et/ou les spectrogrammes. De sorte à ne charger que le nécessaire. Exemple si on veut tout charger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5000\n",
      "Signal's shape:  torch.Size([32, 80000])\n",
      "Spectrogram's shape:  torch.Size([32, 257, 320])\n",
      "SNRs : ('0', '1', '-1', '-1', '2', '0', '0', '1', '0', '3', '3', '3', '-4', '2', '-1', '2', '0', '4', '2', '1', '-3', '-3', '2', '-1', '0', '0', '3', '-1', '0', '-4', '0', '1')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_folder,load_signals=True,load_spectrograms=True)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "print(\"Dataset length:\",len(train_dataset))\n",
    "for voice_signal,voice_spec,noise_signal,noise_spec,mix_signal,mix_spec,snr in train_dataloader:\n",
    "    print(\"Signal's shape: \",voice_signal.shape)\n",
    "    print(\"Spectrogram's shape: \",voice_spec.shape)\n",
    "    print(\"SNRs :\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq using a U-Net: Singing Voice Separation With Deep U-Net Convolutional Networks\n",
    "\n",
    "D'après le [papier de A. Jansson et al](https://openaccess.city.ac.uk/id/eprint/19289/1/7bb8d1600fba70dd79408775cd0c37a4ff62.pdf)\n",
    "\n",
    "Et pour [l'architecture](https://github.com/phillipi/pix2pix/blob/master/models.lua) (qui vient du papier [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(\n",
    "    train_folder,\n",
    "    load_signals=False,\n",
    "    load_spectrograms=True)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spec's shape: torch.Size([32, 257, 320])\n",
      "SNRs: ('-2', '-1', '0', '0', '-4', '2', '1', '-1', '4', '1', '4', '4', '2', '-1', '3', '3', '0', '-4', '0', '-3', '0', '-2', '-2', '0', '0', '-2', '0', '3', '3', '-1', '-4', '-1')\n"
     ]
    }
   ],
   "source": [
    "for voice_spec,noise_spec,mix_spec,snr in train_dataloader:\n",
    "    print(\"Spec's shape:\",voice_spec.shape)\n",
    "    print(\"SNRs:\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Remarque / Astuce</b></font>:\n",
    "Le réseau UNet divise la matrice par des puissances de 2, ici $2^6=64$, puis remultiplie d'autant. Ainsi, pour garder la même dimension en sortie, il est nécessaire que la dimension de la matrice d'entrée soit un multiple de 64. C'est pourquoi pour calculer les spectrogrammes nous prenons un `noverlap=149` (et non 100 comme indiqué dans le cours), ce qui donne $320 = 64*5$ temps d'échantillon (la 2nd dimension). (`noverlap=86` donne $256$ mais n'est pas toujours inversible...). Concernant la 1ère dimension, elle est à $257$: nous oublierons la dernière valeur (ce qui ramène à $256 = 64 * 4$), et en sortie le masque prend la même valeur en fréquence 257 qu'en fréquence 256, pas idéal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dim_257_to_256(x):\n",
    "    return x[...,:256,:]\n",
    "\n",
    "def change_dim_256_to_257(y):\n",
    "    return torch.cat([y,y[...,-1,:].unsqueeze(-2)],-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self,n_levels=5):\n",
    "        super().__init__()\n",
    "        self.n_levels = n_levels\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        ngf = 16\n",
    "        nb_C = [ngf*2**i for i in range(n_levels+1)]\n",
    "\n",
    "        # Partie Descendante\n",
    "        self.down_convs = nn.ModuleList()\n",
    "        self.down_norms = nn.ModuleList()\n",
    "        self.first_conv = nn.Conv2d(1,ngf,5,2,2)\n",
    "        for i in range(n_levels):\n",
    "            self.down_convs.append(nn.Conv2d(\n",
    "                in_channels=nb_C[i],\n",
    "                out_channels=nb_C[i+1],\n",
    "                kernel_size=(5,5),\n",
    "                stride=2,\n",
    "                padding=5//2,\n",
    "            ))\n",
    "            if i < n_levels-1:\n",
    "                self.down_norms.append(nn.BatchNorm2d(nb_C[i+1]))\n",
    "\n",
    "        # Partie Ascendante\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.up_norms = nn.ModuleList()\n",
    "        self.last_conv = nn.ConvTranspose2d(2*ngf,1,5,2,5//2,output_padding=1)\n",
    "        for i in range(n_levels,0,-1):\n",
    "            self.up_convs.append(nn.ConvTranspose2d(\n",
    "                in_channels=2*nb_C[i] if i<n_levels else nb_C[i],\n",
    "                out_channels=nb_C[i-1],\n",
    "                kernel_size=(5,5),\n",
    "                stride=2,\n",
    "                padding=5//2,\n",
    "                output_padding=1\n",
    "            ))\n",
    "            self.up_norms.append(nn.BatchNorm2d(nb_C[i-1]))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x: torch.Tensor,print_shapes=False):\n",
    "        # Preprocess:\n",
    "        mask = change_dim_257_to_256(x) # => Shape (B,256,320)\n",
    "        mask = mask.unsqueeze(1) # => Shape (B,1,256,320)\n",
    "        if print_shapes: print(mask.shape)\n",
    "\n",
    "        # Partie Descendante\n",
    "        mask = self.first_conv(mask)\n",
    "        stack = [mask]\n",
    "        if print_shapes: print(mask.shape)\n",
    "        for i in range(self.n_levels):\n",
    "            mask = self.leaky_relu(mask)\n",
    "            mask = self.down_convs[i](mask)\n",
    "            if i < self.n_levels-1:\n",
    "                mask = self.down_norms[i](mask)\n",
    "                stack.append(mask)\n",
    "            if print_shapes: print(mask.shape)\n",
    "\n",
    "        if print_shapes: print(\"-\")\n",
    "        # Partie Ascendante\n",
    "        for i in range(self.n_levels,0,-1):\n",
    "            if i < self.n_levels:\n",
    "                mask = torch.cat([mask,stack.pop()],dim=1)\n",
    "            mask = torch.relu(mask)\n",
    "            mask = self.up_convs[self.n_levels-i](mask)\n",
    "            mask = self.up_norms[self.n_levels-i](mask)\n",
    "            if self.n_levels-i < 3:\n",
    "                mask = self.drop_out(mask)\n",
    "            if print_shapes: print(mask.shape)\n",
    "        mask = torch.cat([mask,stack.pop()],dim=1)\n",
    "        mask = torch.relu(mask)\n",
    "        mask = self.last_conv(mask)\n",
    "        if print_shapes: print(mask.shape)\n",
    "\n",
    "        # Postprocess:\n",
    "        mask = mask.squeeze(1) # => Shape (B,256,320)\n",
    "        mask = change_dim_256_to_257(mask) # => Shape (B,257,320)\n",
    "        output = mask * x\n",
    "        if print_shapes: print(\"Output shape:\",output.shape)\n",
    "        return mask, output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 320])\n",
      "torch.Size([32, 16, 128, 160])\n",
      "torch.Size([32, 32, 64, 80])\n",
      "torch.Size([32, 64, 32, 40])\n",
      "torch.Size([32, 128, 16, 20])\n",
      "torch.Size([32, 256, 8, 10])\n",
      "torch.Size([32, 512, 4, 5])\n",
      "-\n",
      "torch.Size([32, 256, 8, 10])\n",
      "torch.Size([32, 128, 16, 20])\n",
      "torch.Size([32, 64, 32, 40])\n",
      "torch.Size([32, 32, 64, 80])\n",
      "torch.Size([32, 16, 128, 160])\n",
      "torch.Size([32, 1, 256, 320])\n",
      "Output shape: torch.Size([32, 257, 320])\n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "mask,output = model(voice_spec,print_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([32, 1, 256, 320])\n",
      "Output shape torch.Size([32, 1, 128, 160])\n",
      "z:  torch.Size([32, 1, 256, 320])\n"
     ]
    }
   ],
   "source": [
    "x = voice_spec\n",
    "x = change_dim_257_to_256(x)\n",
    "x = x.unsqueeze(1)\n",
    "conv = torch.nn.Conv2d(1,1,5,2,5//2)\n",
    "print(\"input shape\",x.shape)\n",
    "y = conv(x)\n",
    "print(\"Output shape\",y.shape)\n",
    "unconv = torch.nn.ConvTranspose2d(1,1,5,2,2,output_padding=1)\n",
    "z = unconv(y)\n",
    "print(\"z: \",z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([32, 1, 128])\n",
      "Output shape torch.Size([32, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32,1,128)\n",
    "conv = torch.nn.Conv1d(1,1,3,2,1)\n",
    "print(\"input shape\",x.shape)\n",
    "y = conv(x)\n",
    "print(\"Output shape\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,0,-1):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
