{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.axes._axes import Axes\n",
    "\n",
    "from IPython.display import Audio\n",
    "# Audio(data=signal.T,rate=fe)\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import stft,istft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=6><b>Meta paramètres</b></font>\n",
    "\n",
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute puis les sauvegarder sur le disque. Toutefois, <font color=\"red\"><b>ceci triple l'espace occupé sur disque</b></font>: passant de 6.3 Go à 17.2 Go. Veuillez donc préciser le paramètre `SAVE_SPECTROGRAMS` selon si vous pouvez utiliser cet espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = Path(\"source_separation\")\n",
    "SAVE_SPECTROGRAMS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des signaux, Visualisation et Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des signaux et spectrogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = DIRECTORY / \"train\"\n",
    "train_small_folder = DIRECTORY / \"train_small\"\n",
    "test_folder = DIRECTORY / \"test\"\n",
    "get_path = lambda folder,i : folder / (\"000\"+str(i))[-4:]\n",
    "\n",
    "datasets_sizes = {\n",
    "    train_small_folder : 50,\n",
    "    train_folder : 5000,\n",
    "    test_folder : 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les signaux ont la même fréquence d'échantillonage, même longueur; et donc les mêmes f et t échantillonés pour le Spectrogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_same(folder=train_small_folder):\n",
    "    f_ref = None\n",
    "    t_ref = None\n",
    "    for i in range(datasets_sizes[train_small_folder]):\n",
    "        fe,signal = wavfile.read(get_path(train_small_folder,i) / \"voice.wav\")\n",
    "        len_signal = len(signal)\n",
    "        f_spec,t_spec,spec = stft(\n",
    "            signal,fs=fe,\n",
    "            nperseg=400,nfft=512,noverlap=100)\n",
    "        if f_ref is None: f_ref = f_spec ; t_ref = t_spec\n",
    "        assert np.allclose(f_spec,f_ref)\n",
    "        assert np.allclose(t_spec,t_ref)\n",
    "    return fe,f_ref,t_ref\n",
    "\n",
    "fe,f_ref,t_ref = check_all_same()\n",
    "get_spectrogram = lambda signal : stft(signal,fs=fe,nperseg=400,nfft=512,noverlap=100)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal_folder(folder: Path,\n",
    "        load_signals=True,\n",
    "        load_spectrograms=True) -> dict[str,dict]:\n",
    "    \"\"\"\n",
    "    Return a dictionary with 3 sub dicts: \"voice\", \"noise\" and \"mix\"; and an \"SNR\" key.\n",
    "    Each sub dict has 3 keys: \"filename\", \"signal\" and \"spectrogram\" \n",
    "    (except if load_signals or load_spectrograms are set to False)\n",
    "    \"\"\"\n",
    "    keys = [\"voice\",\"noise\",\"mix\"]\n",
    "    res = dict((k,dict()) for k in keys)\n",
    "    for f in folder.iterdir():\n",
    "        assert f.is_file()\n",
    "        if \"voice\" in f.name: key = \"voice\"\n",
    "        elif \"noise\" in f.name: key = \"noise\"\n",
    "        else: \n",
    "            key = \"mix\"\n",
    "            if f.suffix == \".wav\":\n",
    "                res[\"SNR\"] = f.name.removesuffix(\".wav\").split(\"_\")[-1]\n",
    "        if f.suffix == \".wav\" and load_signals:\n",
    "            fe,signal = wavfile.read(f)\n",
    "            res[key][\"filename\"] = f.name\n",
    "            res[key][\"signal\"] = signal\n",
    "        elif f.suffix == \".pt\" and SAVE_SPECTROGRAMS and load_spectrograms: \n",
    "            # when SAVE_SPECTROGRAMS is False, we shouldn't be able to load them \n",
    "            # to save time, otherwise it's cheating.\n",
    "            res[key][\"spectrogram\"] = torch.load(f,weights_only=True)\n",
    "    # Create missing spectrograms\n",
    "    if load_spectrograms:\n",
    "        for key in keys:\n",
    "            if \"spectrogram\" not in res[key]:\n",
    "                assert load_signals\n",
    "                spec = get_spectrogram(res[key][\"signal\"])\n",
    "                res[key][\"spectrogram\"] = spec\n",
    "                if SAVE_SPECTROGRAMS:\n",
    "                    torch.save(torch.tensor(spec),folder / f\"{key}_spectrogram.pt\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_all_spectrograms():\n",
    "    for folder in [train_folder,train_small_folder,test_folder]:\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            folder_i: Path = get_path(folder,i)\n",
    "            for f in folder_i.iterdir():\n",
    "                if \"spectrogram\" in f.name:\n",
    "                    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme calculer les spectrogrammes prend du temps, nous pouvons les calculer une fois pour toute, en sauvegardant tous les spectrogrammes sur disque. Attention, <font color=\"red\"><b>ceci triple l'espace occupé sur disque</b></font>: passant de 6.3 Go à 17.2 Go.\n",
    "Pour les retirer utiliser: `remove_all_spectrograms()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrograms(folder: Path):\n",
    "    if SAVE_SPECTROGRAMS and not (folder/\"0000\"/\"voice_spectrogram.pt\").exists():\n",
    "        for i in range(datasets_sizes[folder]):\n",
    "            load_signal_folder(get_path(folder,i))\n",
    "\n",
    "create_spectrograms(train_small_folder)\n",
    "create_spectrograms(test_folder)\n",
    "create_spectrograms(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Première fois:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/theotime/ForLinux/theotime/miniconda3/envs/simple/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps total pour load 2000 signaux: 2.8570137719893864\n",
      "Temps total pour en calculer les spectrogrammes: 3.3690152460189893\n",
      "Comparé au temps pour charger les spectrogrammes: 4.343046378005056\n",
      "\n",
      "Deuxième fois:\n",
      "Temps total pour load 2000 signaux: 0.406037850996654\n",
      "Temps total pour en calculer les spectrogrammes: 2.5368334229960965\n",
      "Comparé au temps pour charger les spectrogrammes: 0.7139769720047298\n"
     ]
    }
   ],
   "source": [
    "def compute_time(folder):\n",
    "    time_load_signal = 0\n",
    "    time_load_spec = 0\n",
    "    time_spec = 0\n",
    "    start = time.perf_counter()\n",
    "    for i in range(datasets_sizes[folder]):\n",
    "        fe,signal = wavfile.read(get_path(folder,i) / \"voice.wav\")\n",
    "        time_load_signal += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        path_spec: Path = get_path(folder,i) / \"voice_spectrogram.pt\"\n",
    "        if path_spec.exists():\n",
    "            _ = torch.load(path_spec,weights_only=True)\n",
    "        time_load_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "        _ = get_spectrogram(signal)\n",
    "        time_spec += time.perf_counter() - start ; start = time.perf_counter()\n",
    "\n",
    "    print(f\"Temps total pour load {datasets_sizes[folder]} signaux: {time_load_signal}\")\n",
    "    print(f\"Temps total pour en calculer les spectrogrammes: {time_spec}\")\n",
    "    print(f\"Comparé au temps pour charger les spectrogrammes: {time_load_spec}\")\n",
    "\n",
    "print(\"Première fois:\")\n",
    "compute_time(test_folder)\n",
    "print(\"\\nDeuxième fois:\")\n",
    "compute_time(test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que charger les spectrogrammes pré-calculés est plus rapide que de les calculer à chaque fois. On note aussi une grosse différence entre la première fois qu'un fichier est chargé et la seconde, j'imagine que le système place les derniers fichiers chargés dans le cache (recharger le notebook n'y change rien, donc la différence n'apparait que la toute première fois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Note: les versions très récentes de `ipympl` ont une erreur de frappe dans le code, avec une variable nommé \"buttons\" au lieu de \"button\". Ainsi, si vous avez une version instable de `ipympl`, il se peut que d'un coup la cellule interactive ci-dessous écrivent des dizaines d'erreurs à la chaine. Nous n'avons pas trouvé ce qui les déclenche, mais vous pouvez les ignorer (puisqu'elles n'empechent pas la cellule de tourner), sinon vous pouvez simplement ouvrir le fichier d'où vient l'erreur (en cliquant sur la ligne d'erreur qui s'affiche des dizaines de fois), et changé \"buttons\", par \"button\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5928fc25fde14c12a4ff149117bf8663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='i', max=49), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_signal(folder):\n",
    "    signals = load_signal_folder(folder)\n",
    "    colorbars = []\n",
    "    for ax,name in zip(axs,[\"voice\",\"noise\",\"mix\"]):\n",
    "        spec_dB = 10*np.log10(abs(signals[name][\"spectrogram\"].numpy()))\n",
    "        plt_obj = ax.pcolormesh(\n",
    "            t_ref,f_ref,spec_dB,\n",
    "            vmax=np.percentile(spec_dB,99),\n",
    "            vmin=np.percentile(spec_dB,10))\n",
    "        colorbars.append(plt.colorbar(plt_obj,ax=ax))\n",
    "        ax.set_title(signals[name][\"filename\"])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "        colorbars[-1].ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "        ax.set_xlabel(\"Temps (ms)\",fontsize=4)\n",
    "        ax.set_ylabel(\"Fréquence (GHz)\",fontsize=4)\n",
    "    return colorbars\n",
    "\n",
    "\n",
    "folder = train_small_folder\n",
    "plt.close(\"viz\")\n",
    "fig_viz = plt.figure(\"viz\",figsize=(7,2.5))\n",
    "gs = gridspec.GridSpec(2,3) # to get good control of the color bars\n",
    "ax_slices = [np.s_[:,i] for i in range(3)]\n",
    "axs = [fig_viz.add_subplot(gs[sli]) for sli in ax_slices]\n",
    "colorbars = []\n",
    "\n",
    "@widgets.interact(i=(0,datasets_sizes[folder]-1,1))\n",
    "def update(i=1):\n",
    "    for ax in axs: ax.cla()\n",
    "    global colorbars\n",
    "    fig_viz = plt.figure(\"viz\")\n",
    "    plt.title(\"\")\n",
    "    try: \n",
    "        if colorbars != []:\n",
    "            for colorbar in colorbars:\n",
    "                fig_viz.delaxes(colorbar.ax)\n",
    "            gs = gridspec.GridSpec(2,3)\n",
    "            for ax,sli in zip(axs,ax_slices):\n",
    "                ax.set_position(gs[sli].get_position(fig_viz))\n",
    "                ax.set_subplotspec(gs[sli])\n",
    "    except:\n",
    "        print('got an error')\n",
    "        pass\n",
    "    folder_i = get_path(folder,i)\n",
    "    colorbars = visualize_signal(folder_i)\n",
    "    fig_viz.suptitle(f\"Spectogrammes pour {Path(folder_i.parent.name)/folder_i.name}\")\n",
    "    fig_viz.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "            folder: Path,\n",
    "            load_signals=True,\n",
    "            load_spectrograms=False):\n",
    "        self.folder = folder\n",
    "        self.load_signals = load_signals\n",
    "        self.load_spectrograms = load_spectrograms\n",
    "            \n",
    "    def __len__(self):\n",
    "        return datasets_sizes[self.folder]\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        d = load_signal_folder(\n",
    "            get_path(self.folder,i),\n",
    "            load_signals=self.load_signals,\n",
    "            load_spectrograms=self.load_spectrograms)\n",
    "        ret = []\n",
    "        for name in [\"voice\",\"noise\",\"mix\"]:\n",
    "            if self.load_signals:\n",
    "                ret.append(d[name][\"signal\"])\n",
    "            if self.load_spectrograms:\n",
    "                ret.append(d[name][\"spectrogram\"])\n",
    "        ret.append(d[\"SNR\"])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Dataset peut contenir les signaux et/ou les spectrogrammes. De sorte à ne charger que le nécessaire. Exemple si on veut tout charger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5000\n",
      "Signal's shape:  torch.Size([32, 80000])\n",
      "Spectrogram's shape:  torch.Size([32, 257, 268])\n",
      "SNRs : ('4', '1', '4', '0', '1', '-2', '-1', '-3', '-2', '-2', '2', '3', '-4', '0', '-1', '1', '-4', '2', '0', '-1', '-4', '3', '3', '2', '-4', '1', '-2', '0', '0', '4', '4', '0')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_folder,load_signals=True,load_spectrograms=True)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "print(\"Dataset length:\",len(train_dataset))\n",
    "for voice_signal,voice_spec,noise_signal,noise_spec,mix_signal,mix_spec,snr in train_dataloader:\n",
    "    print(\"Signal's shape: \",voice_signal.shape)\n",
    "    print(\"Spectrogram's shape: \",voice_spec.shape)\n",
    "    print(\"SNRs :\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq using a U-Net: Singing Voice Separation With Deep U-Net Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(\n",
    "    train_folder,\n",
    "    load_signals=False,\n",
    "    load_spectrograms=True)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spec's shape: torch.Size([32, 257, 268])\n",
      "SNRs: ('0', '0', '-2', '3', '0', '0', '-2', '4', '0', '3', '2', '-3', '-1', '1', '1', '-2', '0', '2', '1', '2', '0', '-3', '2', '2', '2', '0', '-2', '-2', '4', '-1', '-2', '-2')\n"
     ]
    }
   ],
   "source": [
    "for voice_spec,noise_spec,mix_spec,snr in train_dataloader:\n",
    "    print(\"Spec's shape:\",voice_spec.shape)\n",
    "    print(\"SNRs:\",snr)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
